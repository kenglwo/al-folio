@string{aps = {American Physical Society,}}

@inproceedings{10.1145/3664647.3681504,
author = {Shigyo, Kento and Cao, Yi-Fan and Takahira, Kentaro and Fan, Mingming and Qu, Huamin},
title = {VR-Mediated Cognitive Defusion: A Comparative Study for Managing Negative Thoughts},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681504},
doi = {10.1145/3664647.3681504},
abstract = {The growing prevalence of psychological disorders underscores the critical importance of mental health research in today's society. In psychotherapy, particularly Acceptance and Commitment Therapy (ACT), cognitive exercises employing mental imagery are used to manage negative thoughts. However, the challenge of maintaining vivid imagery diminishes their therapeutic effectiveness. Virtual reality (VR) offers untapped potential for increasing engagement and therapeutic efficacy. However, there is still a gap in exploration regarding how to effectively leverage the potential of VR to enhance traditional cognitive exercises with mental imagery. This study investigates the effective HCI design and the comparative efficacy of a VR-mediated exercise for promoting cognitive defusion to address negative thoughts grounded in ACT. Using a co-design approach with clinicians and potential users of postgraduate students, we developed a VR system that materializes negative thoughts into tangible objects. This allows users to visually modify and transpose these objects onto a surface, facilitating mental detachment from negative thoughts. In an evaluation study with 20 non-clinical participants, divided into VR and mental imagery groups, we assessed the impact of the cognitive defusion exercise on their perception of negative thoughts and psychological measures using standardized questionnaires. Results show improvement in both groups, with significant enhancements in negative thought perception and mental detachment from negative thoughts exclusively in the VR group, whereas the mental imagery group did not demonstrate significant changes. Interviews emphasize the VR's capability to present vivid visualizations of negative thoughts effortlessly, highlighting its effectiveness and engagement in psychotherapy to facilitate cognitive exercises.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {117–126},
numpages = {10},
keywords = {cognitive defusion, human-computer interaction, psychotherapy, virtual reality},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1109/TVCG.2024.3372104,
author = {Tong, Wai and Shigyo, Kento and Yuan, Lin-Ping and Fan, Mingming and Pong, Ting-Chuen and Qu, Huamin and Xia, Meng},
title = {VisTellAR: Embedding Data Visualization to Short-Form Videos Using Mobile Augmented Reality},
year = {2024},
issue_date = {March 2024},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {31},
number = {3},
issn = {1077-2626},
url = {https://doi.org/10.1109/TVCG.2024.3372104},
doi = {10.1109/TVCG.2024.3372104},
abstract = {With the rise of short-form video platforms and the increasing availability of data, we see the potential for people to share short-form videos embedded with data in situ (e.g., daily steps when running) to increase the credibility and expressiveness of their stories. However, creating and sharing such videos in situ is challenging since it involves multiple steps and skills (e.g., data visualization creation and video editing), especially for amateurs. By conducting a formative study (N=10) using three design probes, we collected the motivations and design requirements. We then built VisTellAR, a mobile AR authoring tool, to help amateur video creators embed data visualizations in short-form videos in situ. A two-day user study shows that participants (N=12) successfully created various videos with data visualizations in situ and they confirmed the ease of use and learning. AR pre-stage authoring was useful to assist people in setting up data visualizations in reality with more designs in camera movements and interaction with gestures and physical objects to storytelling.},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = mar,
pages = {1862–1874},
numpages = {13}
}

@inproceedings{wang2023visualization,
  title={Visualization of potential differences in comprehension by distribution of notes and questions in online programming courses},
  author={Wang, Xiaonan and Kiyomitsu, Hidenari and Su, Yancong and Ohtsuki, Kazuhiro and Sun, Yi and Shigyo, Kento},
  booktitle={2023 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}

@inproceedings{10.1145/3588001.3609364,
author = {Yu, Yue and Yi, Sophia and Nan, Xi and Lo, Leo Yu-Ho and Shigyo, Kento and Xie, Liwenhan and Wicaksana, Jeffry and Cheng, Kwang-Ting and Qu, Huamin},
title = {FoodWise: Food Waste Reduction and Behavior Change on Campus with Data Visualization and Gamification},
year = {2023},
isbn = {9798400701498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588001.3609364},
doi = {10.1145/3588001.3609364},
abstract = {Food waste presents a substantial challenge with significant environmental and economic ramifications, and its severity on campus environments is of particular concern. In response to this, we introduce FoodWise, a dual-component system tailored to inspire and incentivize campus communities to reduce food waste. The system consists of a data storytelling dashboard that graphically displays food waste information from university canteens, coupled with a mobile web application that encourages users to log their food waste reduction actions and rewards active participants for their efforts. Deployed during a two-week food-saving campaign at The Hong Kong University of Science and Technology (HKUST) in March 2023, FoodWise engaged over 200 participants from the university community, resulting in the logging of over 800 daily food-saving actions. Feedback collected post-campaign underscores the system’s efficacy in elevating user consciousness about food waste and prompting behavioral shifts towards a more sustainable campus. This paper also provides insights for enhancing our system, contributing to a broader discourse on sustainable campus initiatives.},
booktitle = {Proceedings of the 6th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {76–83},
numpages = {8},
keywords = {food waste reduction, human-centered computing, persuasive technology, self-tracking, sustainability},
location = {Cape Town, South Africa},
series = {COMPASS '23}
}

@inproceedings{10.1145/3615522.3615578,
author = {Cao, Yifan and Xia, Meng and Shigyo, Kento and Cheng, Furui and Yu, Qianhang and Yang, Xingxing and Wang, Yang and Zeng, Wei and Qu, Huamin},
title = {NFTeller: Dual-centric Visual Analytics for Assessing Market Performance of NFT Collectibles},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615522.3615578},
doi = {10.1145/3615522.3615578},
abstract = {Non-fungible tokens (NFTs) have recently gained widespread popularity as an alternative investment. However, the lack of assessment criteria has caused intense volatility in NFT marketplaces. Identifying attributes impacting the market performance of NFT collectibles is crucial but challenging due to the massive amount of heterogeneous and multi-modal data in NFT transactions, e.g., social media texts, numerical trading data, and images. To address this challenge, we introduce an interactive dual-centric visual analytics system, NFTeller, to facilitate users’ analysis. First, we collaborate with five domain experts to distill static and dynamic impact attributes and collect relevant data. Next, we derive six analysis tasks and develop NFTeller to present the evolution of NFT transactions and correlate NFTs’ market performance with impact attributes. Notably, we create an augmented chord diagram with a radial stacked bar chart to explore intersections between NFT collection projects and whale accounts. Finally, we conduct three case studies and interview domain experts to evaluate the effectiveness and usability of this system. As such, we gain in-depth insights into assessing NFT collectibles and detecting opportune moments for investment.},
booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
articleno = {20},
numpages = {8},
keywords = {Visual analytics, Non-fungible tokens (NFTs), Blockchain},
location = {Guangzhou, China},
series = {VINCI '23}
}

@inproceedings{lo2022misinformed,
  title={Misinformed by visualization: What do we learn from misinformative visualizations?},
  author={Lo, Leo Yu-Ho and Gupta, Ayush and Shigyo, Kento and Wu, Aoyu and Bertini, Enrico and Qu, Huamin},
  booktitle={Computer Graphics Forum},
  volume={41},
  number={3},
  pages={515--525},
  year={2022},
  organization={Wiley Online Library}
}

@INPROCEEDINGS{10148284,
  author={Shigyo, Kento and Kiyomitsu, Hidenari and Sato, Hitoshi and Ohtsuki, Kazuhiro},
  booktitle={2022 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Support Awareness of Anomaly in Coding Behavior using Code Revision Data}, 
  year={2022},
  volume={},
  number={},
  pages={349-355},
  keywords={Codes;Scalability;Education;Syntactics;Encoding;Behavioral sciences;History;programming education;programming novice;Web;HTML},
  doi={10.1109/TALE54877.2022.00064}
}

@INPROCEEDINGS{9001706,
  author={Shigyo, Kento and Kiyomitsu, Hidenari and Ohtsuki, Kazuhiro},
  booktitle={2020 14th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Visualization of Texture Expressions for Recipes by Using Reviews}, 
  year={2020},
  volume={},
  number={},
  pages={1-4},
  keywords={Visualization;Tag clouds;Color;Internet;Speech recognition;Vocabulary;Visualization;Cooking Recipes;Food Texture;Onomatopoeia;Recipe Retrieval},
  doi={10.1109/IMCOM48794.2020.9001706}
}
